# ============================================================
#  NEXUS — AI Video Factory
#  Docker Compose: Kompletny Enterprise Stack
#
#  Uruchomienie: make start  (lub: docker compose up -d)
#  Wszystkie serwisy: make status
#  Pomoc: make help
# ============================================================

x-env-common: &env-common
  SRODOWISKO: ${SRODOWISKO:-development}
  DEBUG: ${DEBUG:-false}
  TAJNY_KLUCZ: ${TAJNY_KLUCZ:-nexus-dev-key-zmien-na-produkcji}
  OPENAI_API_KEY: ${OPENAI_API_KEY}
  LANGCHAIN_TRACING_V2: ${LANGCHAIN_TRACING_V2:-false}
  LANGCHAIN_API_KEY: ${LANGCHAIN_API_KEY:-}
  LANGCHAIN_PROJECT: ${LANGCHAIN_PROJECT:-nexus-ai-platforma}

x-backend-common: &backend-common
  build:
    context: ./backend
    dockerfile: Dockerfile
    args:
      BUILDKIT_INLINE_CACHE: 1
  environment:
    <<: *env-common
    POSTGRES_URL: postgresql+asyncpg://nexus:${POSTGRES_PASSWORD:-nexus2026}@postgres:5432/nexus
    REDIS_URL: redis://:${REDIS_PASSWORD:-nexus2026}@redis:6379/0
    CHROMA_SCIEZKA: /app/dane/chroma
    SCIEZKA_TYMCZASOWA: /tmp/nexus
    SCIEZKA_WYJSCIOWA: /app/dane/wideo
    DOZWOLONE_ZRODLA: '["http://localhost","http://localhost:3000","http://nginx"]'
  volumes:
    - nexus-dane:/app/dane
    - nexus-temp:/tmp/nexus
  depends_on:
    postgres:
      condition: service_healthy
    redis:
      condition: service_healthy
  networks:
    - nexus-internal
    - nexus-public

services:
  # ─────────────────────────────────────────────────────────
  #  NGINX — Reverse Proxy (punkt wejścia, port 80)
  # ─────────────────────────────────────────────────────────
  nginx:
    image: nginx:1.27-alpine
    container_name: nexus-nginx
    restart: unless-stopped
    ports:
      - "80:80"
    volumes:
      - ./nginx/nexus.conf:/etc/nginx/conf.d/default.conf:ro
      - nginx-logs:/var/log/nginx
    depends_on:
      - backend
      - frontend
    networks:
      - nexus-public
    healthcheck:
      test: ["CMD", "nginx", "-t"]
      interval: 30s
      timeout: 5s
      retries: 3

  # ─────────────────────────────────────────────────────────
  #  BACKEND — FastAPI (API serwer)
  # ─────────────────────────────────────────────────────────
  backend:
    <<: *backend-common
    container_name: nexus-backend
    restart: unless-stopped
    command: >
      uvicorn api.main:app
      --host 0.0.0.0
      --port 8000
      --workers 2
      --loop uvloop
      --http httptools
      --log-level info
      --access-log
    expose:
      - "8000"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/api/zdrowie"]
      interval: 20s
      timeout: 8s
      retries: 4
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: "3.0"
          memory: 3G
        reservations:
          cpus: "0.5"
          memory: 512M
    labels:
      - "nexus.service=api"
      - "nexus.description=FastAPI Backend"

  # ─────────────────────────────────────────────────────────
  #  CELERY WORKER — Background Video Generation
  # ─────────────────────────────────────────────────────────
  celery-worker:
    <<: *backend-common
    container_name: nexus-celery-worker
    restart: unless-stopped
    command: >
      celery -A celery_app worker
      --loglevel=info
      --concurrency=2
      --queues=wideo,analityka
      --hostname=worker@%%h
      --max-tasks-per-child=50
      --prefetch-multiplier=1
    depends_on:
      backend:
        condition: service_healthy
      redis:
        condition: service_healthy
    healthcheck:
      # Bez -d: broadcast do wszystkich workerów przez Redis.
      # -d worker@localhost było błędne — hostname to worker@<container-id>,
      # więc health check zawsze failował mimo że worker działał.
      test: ["CMD", "celery", "-A", "celery_app", "inspect", "ping", "--timeout=10"]
      interval: 60s
      timeout: 20s
      retries: 3
      start_period: 40s
    deploy:
      resources:
        limits:
          cpus: "4.0"       # FFmpeg może używać wielu rdzeni
          memory: 6G        # DALL-E obrazy mogą być duże w pamięci
        reservations:
          cpus: "1.0"
          memory: 1G
    labels:
      - "nexus.service=worker"
      - "nexus.description=Celery Video Generation Worker"

  # ─────────────────────────────────────────────────────────
  #  CELERY BEAT — Scheduler (zaplanowane zadania)
  # ─────────────────────────────────────────────────────────
  celery-beat:
    <<: *backend-common
    container_name: nexus-celery-beat
    restart: unless-stopped
    command: >
      celery -A celery_app beat
      --loglevel=info
      --schedule=/app/dane/celerybeat-schedule.db
    depends_on:
      celery-worker:
        condition: service_started
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 256M
    labels:
      - "nexus.service=scheduler"

  # ─────────────────────────────────────────────────────────
  #  FLOWER — Celery Monitoring Dashboard (port 5555)
  # ─────────────────────────────────────────────────────────
  flower:
    <<: *backend-common
    container_name: nexus-flower
    restart: unless-stopped
    command: >
      celery -A celery_app flower
      --port=5555
      --max_tasks=1000
      --purge_offline_workers=60
      --persistent=True
      --db=/app/dane/flower.db
    ports:
      - "5555:5555"
    environment:
      <<: *env-common
      POSTGRES_URL: postgresql+asyncpg://nexus:${POSTGRES_PASSWORD:-nexus2026}@postgres:5432/nexus
      REDIS_URL: redis://:${REDIS_PASSWORD:-nexus2026}@redis:6379/0
      CHROMA_SCIEZKA: /app/dane/chroma
      SCIEZKA_TYMCZASOWA: /tmp/nexus
      SCIEZKA_WYJSCIOWA: /app/dane/wideo
      FLOWER_BASIC_AUTH: ${FLOWER_USER:-nexus}:${FLOWER_PASSWORD:-nexus2026}
    depends_on:
      - celery-worker
    networks:
      - nexus-internal
      - nexus-public
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5555"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 256M

  # ─────────────────────────────────────────────────────────
  #  FRONTEND — Next.js 15 (port 3000 wewnętrzny)
  # ─────────────────────────────────────────────────────────
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
      args:
        BUILDKIT_INLINE_CACHE: 1
    container_name: nexus-frontend
    restart: unless-stopped
    expose:
      - "3000"
    environment:
      NODE_ENV: production
      NEXT_PUBLIC_API_URL: http://localhost/api
      NEXT_PUBLIC_WS_URL: ws://localhost/ws
      BACKEND_URL: http://backend:8000
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    networks:
      - nexus-public
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 512M
    labels:
      - "nexus.service=frontend"

  # ─────────────────────────────────────────────────────────
  #  POSTGRESQL — Baza danych transakcyjnych
  # ─────────────────────────────────────────────────────────
  postgres:
    image: postgres:17-alpine
    container_name: nexus-postgres
    restart: unless-stopped
    environment:
      POSTGRES_DB: nexus
      POSTGRES_USER: nexus
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-nexus2026}
      POSTGRES_INITDB_ARGS: "--encoding=UTF-8 --locale=pl_PL.UTF-8"
      # Optymalizacja dla laptopa (zamiast domyślnych)
      POSTGRES_HOST_AUTH_METHOD: scram-sha-256
    ports:
      - "5432:5432"
    volumes:
      - postgres-dane:/var/lib/postgresql/data
      - ./scripts/init-db.sql:/docker-entrypoint-initdb.d/01-init.sql:ro
    command: >
      postgres
      -c max_connections=50
      -c shared_buffers=256MB
      -c effective_cache_size=1GB
      -c work_mem=16MB
      -c maintenance_work_mem=64MB
      -c random_page_cost=1.1
      -c checkpoint_completion_target=0.9
      -c wal_buffers=16MB
      -c default_statistics_target=100
      -c log_min_duration_statement=500
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U nexus -d nexus"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 20s
    networks:
      - nexus-internal
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 1G
    labels:
      - "nexus.service=database"

  # ─────────────────────────────────────────────────────────
  #  REDIS — Cache + Celery Broker (port 6379)
  # ─────────────────────────────────────────────────────────
  redis:
    image: redis:7.4-alpine
    container_name: nexus-redis
    restart: unless-stopped
    command: >
      redis-server
      --requirepass ${REDIS_PASSWORD:-nexus2026}
      --appendonly yes
      --appendfsync everysec
      --maxmemory 1gb
      --maxmemory-policy allkeys-lru
      --save 900 1
      --save 300 10
      --tcp-backlog 128
      --timeout 300
      --tcp-keepalive 60
      --loglevel notice
    ports:
      - "6379:6379"
    volumes:
      - redis-dane:/data
    healthcheck:
      test: ["CMD", "redis-cli", "-a", "${REDIS_PASSWORD:-nexus2026}", "ping"]
      interval: 10s
      timeout: 3s
      retries: 5
    networks:
      - nexus-internal
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 1.2G

  # ─────────────────────────────────────────────────────────
  #  PROMETHEUS — Metryki (port 9090)
  # ─────────────────────────────────────────────────────────
  prometheus:
    image: prom/prometheus:v3.1.0
    container_name: nexus-prometheus
    restart: unless-stopped
    command:
      - --config.file=/etc/prometheus/prometheus.yml
      - --storage.tsdb.path=/prometheus
      - --storage.tsdb.retention.time=15d
      - --storage.tsdb.retention.size=5GB
      - --web.console.libraries=/etc/prometheus/console_libraries
      - --web.console.templates=/etc/prometheus/consoles
      - --web.enable-lifecycle
      - --web.enable-admin-api
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus-dane:/prometheus
    networks:
      - nexus-internal
      - nexus-public
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 512M

  # ─────────────────────────────────────────────────────────
  #  GRAFANA — Dashboardy (port 3001)
  # ─────────────────────────────────────────────────────────
  grafana:
    image: grafana/grafana:11.4.0
    container_name: nexus-grafana
    restart: unless-stopped
    ports:
      - "3001:3000"
    environment:
      GF_SECURITY_ADMIN_USER: admin
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_PASSWORD:-nexus}
      GF_USERS_ALLOW_SIGN_UP: false
      GF_SERVER_DOMAIN: localhost
      GF_ANALYTICS_REPORTING_ENABLED: false
      GF_PLUGINS_ALLOW_LOADING_UNSIGNED_PLUGINS: grafana-piechart-panel
      GF_DASHBOARDS_DEFAULT_HOME_DASHBOARD_PATH: /var/lib/grafana/dashboards/nexus.json
    volumes:
      - grafana-dane:/var/lib/grafana
      - ./grafana/provisioning:/etc/grafana/provisioning:ro
      - ./grafana/dashboards:/var/lib/grafana/dashboards:ro
    depends_on:
      - prometheus
    networks:
      - nexus-internal
      - nexus-public
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 256M

# ─────────────────────────────────────────────────────────
#  VOLUMES — Persystencja danych
# ─────────────────────────────────────────────────────────
volumes:
  nexus-dane:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${PWD}/dane
  nexus-temp:
    driver: local
  postgres-dane:
    driver: local
  redis-dane:
    driver: local
  prometheus-dane:
    driver: local
  grafana-dane:
    driver: local
  nginx-logs:
    driver: local

# ─────────────────────────────────────────────────────────
#  SIECI
# ─────────────────────────────────────────────────────────
networks:
  nexus-internal:
    name: nexus-internal
    driver: bridge
    internal: true     # Brak dostępu do internetu (baza, redis)
  nexus-public:
    name: nexus-public
    driver: bridge     # Dostęp do internetu (backend, nginx, grafana)
